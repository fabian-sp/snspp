% Encoding: UTF-8

@Article{Bertsekas2011,
  author    = {Dimitri P. Bertsekas},
  journal   = {Mathematical Programming},
  title     = {Incremental proximal methods for large scale convex optimization},
  year      = {2011},
  month     = {jun},
  number    = {2},
  pages     = {163--195},
  volume    = {129},
  doi       = {10.1007/s10107-011-0472-0},
  file      = {:/home/fabian/Dropbox/Promotion/stochastic_ssnal/bertsekas_incremental.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Davis2019,
  author    = {Damek Davis and Dmitriy Drusvyatskiy},
  journal   = {{SIAM} Journal on Optimization},
  title     = {Stochastic Model-Based Minimization of Weakly Convex Functions},
  year      = {2019},
  month     = {jan},
  number    = {1},
  pages     = {207--239},
  volume    = {29},
  doi       = {10.1137/18m1178244},
  file      = {:18m1178244.pdf:PDF},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Article{Asi2019,
  author    = {Hilal Asi and John C. Duchi},
  journal   = {{SIAM} Journal on Optimization},
  title     = {Stochastic (Approximate) Proximal Point Methods: Convergence, Optimality, and Adaptivity},
  year      = {2019},
  month     = {jan},
  number    = {3},
  pages     = {2257--2290},
  volume    = {29},
  doi       = {10.1137/18m1230323},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Article{Toulis2017,
  author    = {Panos Toulis and Edoardo M. Airoldi},
  journal   = {The Annals of Statistics},
  title     = {Asymptotic and finite-sample properties of estimators based on stochastic gradients},
  year      = {2017},
  month     = {aug},
  number    = {4},
  pages     = {1694--1727},
  volume    = {45},
  doi       = {10.1214/16-aos1506},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Patrascu2017,
  author      = {Andrei Patrascu and Ion Necoara},
  title       = {Nonasymptotic convergence of stochastic proximal point algorithms for constrained convex optimization},
  year        = {2017},
  abstract    = {A very popular approach for solving stochastic optimization problems is the stochastic gradient descent method (SGD). Although the SGD iteration is computationally cheap and the practical performance of this method may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate parameters choice. To avoid these drawbacks naturally introduced by the SGD scheme, the stochastic proximal point algorithms have been recently considered in the literature. We introduce a new variant of the stochastic proximal point method (SPP) for solving stochastic convex optimization problems subject to (in)finite intersection of constraints satisfying a linear regularity type condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex and Lipschitz continuous objective functions, we prove nonasymptotic estimates for the rate of convergence in terms of the expected value function gap of order $\mathcal{O}(1/k^{1/2})$, where $k$ is the iteration counter. We also derive better nonasymptotic bounds for the rate of convergence in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order $\mathcal{O}(1/k)$. Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP method that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real-world problems.},
  date        = {2017-06-20},
  eprint      = {1706.06297v1},
  eprintclass = {math.OC},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1706.06297v1:PDF},
  keywords    = {math.OC},
}

@Article{Luo2019,
  author      = {Luo Luo and Cheng Chen and Yujun Li and Guangzeng Xie and Zhihua Zhang},
  title       = {A Stochastic Proximal Point Algorithm for Saddle-Point Problems},
  year        = {2019},
  abstract    = {We consider saddle point problems which objective functions are the average of $n$ strongly convex-concave individual components. Recently, researchers exploit variance reduction methods to solve such problems and achieve linear-convergence guarantees. However, these methods have a slow convergence when the condition number of the problem is very large. In this paper, we propose a stochastic proximal point algorithm, which accelerates the variance reduction method SAGA for saddle point problems. Compared with the catalyst framework, our algorithm reduces a logarithmic term of condition number for the iteration complexity. We adopt our algorithm to policy evaluation and the empirical results show that our method is much more efficient than state-of-the-art methods.},
  date        = {2019-09-13},
  eprint      = {1909.06946v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1909.06946v1:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
}

@Unpublished{Boyd2014,
  author = {Stephen Boyd and Ernest K Ryu},
  title  = {Stochastic proximal iteration: a non-asymptotic improvement upon stochastic gradient descent},
  year   = {2014},
}

@Book{Beck2017,
  author    = {Amir Beck},
  publisher = {Society for Industrial and Applied Mathematics},
  title     = {First-Order Methods in Optimization},
  year      = {2017},
  month     = {oct},
  doi       = {10.1137/1.9781611974997},
}

@Book{Bauschke2011,
  author    = {Heinz H. Bauschke and Patrick L. Combettes},
  publisher = {Springer New York},
  title     = {Convex Analysis and Monotone Operator Theory in Hilbert Spaces},
  year      = {2011},
  doi       = {10.1007/978-1-4419-9467-7},
}

@Book{HiriartUrruty1993,
  author    = {Jean-Baptiste Hiriart-Urruty and Claude Lemar{\'{e}}chal},
  publisher = {Springer Berlin Heidelberg},
  title     = {Convex Analysis and Minimization Algorithms {II}},
  year      = {1993},
  doi       = {10.1007/978-3-662-06409-2},
}

@Article{Jiang1995,
  author    = {H.Y. Jiang and L.Q. Qi},
  journal   = {Journal of Mathematical Analysis and Applications},
  title     = {Local Uniqueness and Convergence of Iterative Methods for Nonsmooth Variational Inequalities},
  year      = {1995},
  month     = {nov},
  number    = {1},
  pages     = {314--331},
  volume    = {196},
  doi       = {10.1006/jmaa.1995.1412},
  file      = {:/home/fabian/Dropbox/Promotion/stochastic_ssnal/jiang_clarke_monotone.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Book{Ulbrich2011,
  author    = {Michael Ulbrich},
  publisher = {Society for Industrial and Applied Mathematics},
  title     = {Semismooth Newton Methods for Variational Inequalities and Constrained Optimization Problems in Function Spaces},
  year      = {2011},
  month     = {jan},
  doi       = {10.1137/1.9781611970692},
}

@Article{Ghadimi2013,
  author    = {Saeed Ghadimi and Guanghui Lan},
  journal   = {{SIAM} Journal on Optimization},
  title     = {Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming},
  year      = {2013},
  month     = {jan},
  number    = {4},
  pages     = {2341--2368},
  volume    = {23},
  doi       = {10.1137/120880811},
  file      = {:/home/fabian/Dropbox/Promotion/stochastic_ssnal/ghadimi_sfo.pdf:PDF},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Article{Bertsekas1973,
  author    = {D. P. Bertsekas},
  journal   = {Journal of Optimization Theory and Applications},
  title     = {Stochastic optimization problems with nondifferentiable cost functionals},
  year      = {1973},
  month     = {aug},
  number    = {2},
  pages     = {218--231},
  volume    = {12},
  doi       = {10.1007/bf00934819},
  file      = {:/home/fabian/Dropbox/Promotion/stochastic_ssnal/berstekas_stochastic_opt.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Zhao2010,
  author    = {Xin-Yuan Zhao and Defeng Sun and Kim-Chuan Toh},
  journal   = {{SIAM} Journal on Optimization},
  title     = {A Newton-{CG} Augmented Lagrangian Method for Semidefinite Programming},
  year      = {2010},
  month     = {jan},
  number    = {4},
  pages     = {1737--1765},
  volume    = {20},
  doi       = {10.1137/080718206},
  file      = {:/home/fabian/Dropbox/Promotion/Master Thesis/Literatur/Referenced/from GGL/47_from_16_sun_newton_cg_converegnce.pdf:PDF},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Article{Li2018,
  author    = {Xudong Li and Defeng Sun and Kim-Chuan Toh},
  journal   = {{SIAM} Journal on Optimization},
  title     = {A Highly Efficient Semismooth Newton Augmented Lagrangian Method for Solving Lasso Problems},
  year      = {2018},
  month     = {jan},
  number    = {1},
  pages     = {433--458},
  volume    = {28},
  doi       = {10.1137/16m1097572},
  file      = {:/home/fabian/Dropbox/Promotion/stochastic_ssnal/sun_ssnal.pdf:PDF},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Book{Facchinei2004,
  editor    = {Francisco Facchinei and Jong-Shi Pang},
  publisher = {Springer New York},
  title     = {Finite-Dimensional Variational Inequalities and Complementarity Problems},
  year      = {2004},
  doi       = {10.1007/b97544},
}

@Book{Nesterov2018,
  author    = {Yurii Nesterov},
  publisher = {Springer International Publishing},
  title     = {Lectures on Convex Optimization},
  year      = {2018},
  doi       = {10.1007/978-3-319-91578-4},
}

@Article{Pang1995,
  author    = {J. S. Pang and L. Qi},
  journal   = {Journal of Optimization Theory and Applications},
  title     = {A globally convergent Newton method for convex {SC}1 minimization problems},
  year      = {1995},
  month     = {jun},
  number    = {3},
  pages     = {633--648},
  volume    = {85},
  doi       = {10.1007/bf02193060},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Defazio2014,
  author      = {Aaron Defazio and Francis Bach and Simon Lacoste-Julien},
  title       = {SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
  abstract    = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
  date        = {2014-07-01},
  eprint      = {1407.0202v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1407.0202v3:PDF},
  keywords    = {cs.LG, math.OC, stat.ML},
}

@Comment{jabref-meta: databaseType:bibtex;}
